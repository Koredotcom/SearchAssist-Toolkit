{
  "cragEvaluationPrompt": "# Task: \\r\\nYou are given a Question, a model Prediction, and a list of Ground Truth answers, judge whether the model Prediction matches any answer from the list of Ground Truth answers. Follow the instructions step by step to make a judgement. \\r\\n1. If the model prediction matches any provided answers from the Ground Truth Answer list, \\\"Accuracy\\\" should be \\\"True\\\"; otherwise, \\\"Accuracy\\\" should be \\\"False\\\".\\r\\n2. If the model prediction says that it couldn't answer the question or it doesn't have enough information, \\\"Accuracy\\\" should always be \\\"False\\\".\\r\\n3. If the Ground Truth is \\\"invalid question\\\", \\\"Accuracy\\\" is \\\"True\\\" only if the model prediction is exactly \\\"invalid question\\\".\\r\\n# Output: \\r\\nRespond with only a single JSON string with an \\\"Accuracy\\\" field which is \\\"True\\\" or \\\"False\\\".\\r\\n# Examples:\\r\\nQuestion: how many seconds is 3 minutes 15 seconds?\\r\\nGround truth: [\\\"195 seconds\\\"]\\r\\nPrediction: 3 minutes 15 seconds is 195 seconds.\\r\\nAccuracy: True\\r\\n\\r\\nQuestion: Who authored The Taming of the Shrew (published in 2002)?\\r\\nGround truth: [\\\"William Shakespeare\\\", \\\"Roma Gill\\\"]\\r\\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare.\\r\\nAccuracy: False\\r\\n\\r\\nQuestion: Who played Sheldon in Big Bang Theory?\\r\\nGround truth: [\\\"Jim Parsons\\\", \\\"Iain Armitage\\\"]\\r\\nPrediction: I am sorry I don't know.\\r\\nAccuracy: False",
  
  "llmEvaluator": {
    "answerRelevancy": {
      "system": "You are a helpful and precise evaluator tasked with assessing the relevancy of an answer generated by a Retrieval-Augmented Generation (RAG) system. Your role is to compare the generated answer with the user query and assign a relevancy score from 1 to 5, based on how well the answer aligns with the user's intent and information need. Focus solely on whether the answer is relevant and responsive to the query, regardless of supporting context or ground truth.",
      "user": "Please evaluate the following answer for relevancy to the given query, using the rubric below:\\n\\n---\\n\\nüìå **Rubric for Answer Relevancy (1‚Äì5):**\\n\\n**5 - Fully Relevant:** The answer completely and directly addresses the user query, with no off-topic or irrelevant content.\\n\\n**4 - Mostly Relevant:** The answer is strongly related to the query and mostly fulfills its intent, with only minor off-topic details or slight undercoverage.\\n\\n**3 - Somewhat Relevant:** The answer has partial relevance. It touches on the topic but misses important aspects of the intent or includes moderately unrelated content.\\n\\n**2 - Weakly Relevant:** The answer is only loosely related to the query and fails to meet the query's intent. It contains significant irrelevant content.\\n\\n**1 - Not Relevant:** The answer is completely unrelated to the query.\\n\\n---\\n\\n‚úâÔ∏è **User Query:**\\n{user_query}\\n\\nüßæ **RAG Generated Answer:**\\n{generated_answer}\\n\\n---\\n\\nüîç Please return your evaluation in the following format:\\n```json\\n{{\\n  \\\"score\\\": <1-5>,\\n  \\\"justification\\\": \\\"<Brief explanation of your rating>\\\"\\n}}\\n```"
    },
    "contextRelevancy": {
      "system": "You are a skilled evaluator tasked with judging the relevancy of retrieved context in response to a user query. Your goal is to assess how relevant the retrieved context is to the query, on a scale from 1 to 5, based on how well it supports answering the query. Focus only on the alignment between the context and the query ‚Äî not on the final answer.",
      "user": "Evaluate the relevance of the retrieved context with respect to the given query using the following rubric:\\n\\n---\\n\\nüìå **Rubric for Context Relevancy (1‚Äì5):**\\n\\n**5 - Highly Relevant:** The context directly supports answering the core intent of the query with high specificity and usefulness.\\n\\n**4 - Mostly Relevant:** The context covers most aspects of the query, with minor gaps or generalizations.\\n\\n**3 - Somewhat Relevant:** The context is partially related to the query but lacks specificity or focus; helpful to some extent.\\n\\n**2 - Weakly Relevant:** The context contains only loose or tangential references to the query, offering little support.\\n\\n**1 - Not Relevant:** The context is unrelated or off-topic and does not help in answering the query.\\n\\n---\\n\\n‚úâÔ∏è **User Query:**\\n{user_query}\\n\\nüìö **Retrieved Context:**\\n{retrieved_context}\\n\\n---\\n\\nüîç Please return your evaluation in the following format:\\n```json\\n{{\\n  \\\"score\\\": <1-5>,\\n  \\\"justification\\\": \\\"<Brief explanation of your rating>\\\"\\n}}\\n```"
    },
    "answerCorrectness": {
      "system": "You are an expert evaluator assessing the correctness of an answer generated by a Retrieval-Augmented Generation (RAG) system. Your goal is to determine how accurately the generated answer aligns with the ground truth and how well it addresses the user's original query. You will assign a score from 1 (Completely Incorrect) to 5 (Completely Correct), using a detailed rubric.",
      "user": "Given a user query, a ground truth answer, and a RAG-generated answer, please evaluate the correctness of the generated answer using the rubric below:\\n\\nüéØ **Rubric for Answer Correctness**\\n\\n**5 - Completely Correct:** The generated answer fully matches the ground truth in meaning and detail. It accurately answers the query without introducing any errors or omissions.\\n\\n**4 - Mostly Correct:** The generated answer is largely accurate and aligns closely with the ground truth, with only minor omissions or slight differences in wording that do not affect the core meaning.\\n\\n**3 - Partially Correct:** The answer includes some correct information relevant to the query and ground truth, but misses important points or includes minor factual inaccuracies.\\n\\n**2 - Weakly Correct:** The answer contains fragments of relevant information but is mostly incorrect or significantly incomplete compared to the ground truth.\\n\\n**1 - Completely Incorrect:** The answer does not match the ground truth at all. It is factually wrong, irrelevant, or misleading with respect to the query.\\n\\n---\\n\\nüì• **Input Format**\\n\\n**User Query:**\\n{user_query}\\n\\n**Ground Truth Answer:**\\n{ground_truth_answer}\\n\\n**RAG Generated Answer:**\\n{generated_answer}\\n\\n---\\n\\nüß† **Instructions**\\n- Carefully read the query, ground truth, and generated answer.\\n- Compare the generated answer with the ground truth, considering how well it answers the original query.\\n- Assign a correctness score (1‚Äì5) based on the rubric above.\\n- Provide a brief justification (1‚Äì2 sentences) for your score.\\n\\n---\\n\\nüìù **Output Format**\\n```json\\n{{\\n  \\\"score\\\": <1-5>,\\n  \\\"justification\\\": \\\"<Brief explanation of your rating>\\\"\\n}}\\n```"
    }
  }
}