{
  "cragEvaluationPrompt": "# Task: \\r\\nYou are given a Question, a model Prediction, and a list of Ground Truth answers, judge whether the model Prediction matches any answer from the list of Ground Truth answers. Follow the instructions step by step to make a judgement. \\r\\n1. If the model prediction matches any provided answers from the Ground Truth Answer list, \\\"Accuracy\\\" should be \\\"True\\\"; otherwise, \\\"Accuracy\\\" should be \\\"False\\\".\\r\\n2. If the model prediction says that it couldn't answer the question or it doesn't have enough information, \\\"Accuracy\\\" should always be \\\"False\\\".\\r\\n3. If the Ground Truth is \\\"invalid question\\\", \\\"Accuracy\\\" is \\\"True\\\" only if the model prediction is exactly \\\"invalid question\\\".\\r\\n# Output: \\r\\nRespond with only a single JSON string with an \\\"Accuracy\\\" field which is \\\"True\\\" or \\\"False\\\".\\r\\n# Examples:\\r\\nQuestion: how many seconds is 3 minutes 15 seconds?\\r\\nGround truth: [\\\"195 seconds\\\"]\\r\\nPrediction: 3 minutes 15 seconds is 195 seconds.\\r\\nAccuracy: True\\r\\n\\r\\nQuestion: Who authored The Taming of the Shrew (published in 2002)?\\r\\nGround truth: [\\\"William Shakespeare\\\", \\\"Roma Gill\\\"]\\r\\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare.\\r\\nAccuracy: False\\r\\n\\r\\nQuestion: Who played Sheldon in Big Bang Theory?\\r\\nGround truth: [\\\"Jim Parsons\\\", \\\"Iain Armitage\\\"]\\r\\nPrediction: I am sorry I don't know.\\r\\nAccuracy: False",
  "relevanceEvaluationPrompt": "You are an expert evaluator helping assess the quality of information retrieval for question answering systems.\\r\\n\\r\\nGiven a user query and a retrieved context passage, your task is to evaluate:\\r\\n\\r\\n1. **Relevance**: How directly the context relates to the query.\\r\\n2. **Usefulness**: How helpful this context would be in answering the query.\\r\\n\\r\\nPlease return:\\r\\n- A **Relevance score** from 0 to 5, where:\\r\\n    - 0 = Completely irrelevant\\r\\n    - 1 = Slightly related\\r\\n    - 2 = Moderately related\\r\\n    - 3 = Related but lacking detail\\r\\n    - 4 = Mostly relevant and partially helpful\\r\\n    - 5 = Highly relevant and directly helpful for answering the query\\r\\n\\r\\n- A **short explanation** justifying your score.\\r\\n\\r\\n---\\r\\n\\r\\n**Query:**\\r\\n{query}\\r\\n\\r\\n**Retrieved Context:**\\r\\n{context_chunk}\\r\\n\\r\\n---\\r\\n\\r\\nYour response should be in this exact format:\\r\\n\\r\\nRelevance Score: [0–5]\\r\\nExplanation: [1–3 sentences explaining your score]"
}

  