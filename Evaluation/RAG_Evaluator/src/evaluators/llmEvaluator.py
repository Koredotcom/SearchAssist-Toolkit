import os
import json
import asyncio
import aiohttp
import time
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from .baseEvaluator import BaseEvaluator
import logging

logger = logging.getLogger(__name__)

class LLMEvaluator(BaseEvaluator):
    """
    LLM-based evaluator using OpenAI to assess Answer Correctness, Answer Relevancy, and Context Relevancy
    """
    
    def __init__(self, openai_config: Dict[str, Any] = None, azure_config: Dict[str, Any] = None):
        super().__init__()
        self.openai_config = openai_config or {}
        self.azure_config = azure_config or {}
        
        logger.info("üîß Initializing LLM Evaluator...")
        logger.info(f"üìã OpenAI config keys: {list(self.openai_config.keys())}")
        logger.info(f"üìã Azure config keys: {list(self.azure_config.keys())}")
        
        # Load prompts from JSON file
        self.prompts = self._load_prompts()
        
        # Check configuration validity
        has_valid_openai = self._has_valid_openai_config()
        has_valid_azure = self._has_valid_azure_config()
        user_selected_openai = self._user_selected_openai_model()
        
        logger.info(f"‚úÖ Valid OpenAI config: {has_valid_openai}")
        logger.info(f"‚úÖ Valid Azure config: {has_valid_azure}")
        logger.info(f"üéØ User selected OpenAI model: {user_selected_openai}")
        
        self.api_key = self._get_api_key()
        self.model_name = self._get_model_name()
        self.base_url = self._get_base_url()
        self.headers = self._get_headers()
        
        # Validation
        if not self.api_key:
            logger.error("‚ùå No valid API key found in configuration or environment")
            raise ValueError("API key is required for LLM Evaluator. Please provide valid OpenAI or Azure OpenAI credentials.")
        
        logger.info(f"ü§ñ LLM Evaluator initialized successfully!")
        logger.info(f"üéØ Using model: {self.model_name}")
        logger.info(f"üåê Using endpoint: {self.base_url.split('?')[0]}...")  # Don't log full URL with params
    
    def debug_configuration(self) -> Dict[str, Any]:
        """Debug method to show current configuration choices"""
        return {
            "has_valid_openai_config": self._has_valid_openai_config(),
            "has_valid_azure_config": self._has_valid_azure_config(),
            "user_selected_openai_model": self._user_selected_openai_model(),
            "api_key_source": "OpenAI" if self.api_key == self.openai_config.get('api_key') else "Azure" if self.api_key == self.azure_config.get('api_key') else "Environment",
            "model_name": self.model_name,
            "endpoint_type": "Azure" if "azure.com" in self.base_url or "deployments" in self.base_url else "OpenAI",
            "base_url": self.base_url.split('?')[0]  # Don't expose full URL
        }
    
    def _load_prompts(self) -> Dict[str, Any]:
        """Load prompts from the prompts.json file"""
        try:
            # Get the directory of the current file
            current_dir = os.path.dirname(os.path.abspath(__file__))
            # Navigate to the prompts directory
            prompts_dir = os.path.join(current_dir, '..', 'prompts')
            prompts_file = os.path.join(prompts_dir, 'prompts.json')
            
            with open(prompts_file, 'r', encoding='utf-8') as f:
                prompts_data = json.load(f)
            
            # Extract LLM evaluator prompts
            llm_prompts = prompts_data.get('llmEvaluator', {})
            if not llm_prompts:
                logger.warning("‚ö†Ô∏è No LLM evaluator prompts found in prompts.json, using default prompts")
                return self._get_default_prompts()
            
            logger.info("‚úÖ Successfully loaded prompts from prompts.json")
            return llm_prompts
            
        except FileNotFoundError:
            logger.error(f"‚ùå Prompts file not found at {prompts_file}")
            logger.info("üîÑ Using default prompts")
            return self._get_default_prompts()
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Error parsing prompts.json: {e}")
            logger.info("üîÑ Using default prompts")
            return self._get_default_prompts()
        except Exception as e:
            logger.error(f"‚ùå Error loading prompts: {e}")
            logger.info("üîÑ Using default prompts")
            return self._get_default_prompts()
    
    def _get_default_prompts(self) -> Dict[str, Any]:
        """Return default prompts as fallback"""
        return {
            "answerRelevancy": {
                "system": "You are a helpful and precise evaluator tasked with assessing the relevancy of an answer generated by a Retrieval-Augmented Generation (RAG) system. Your role is to compare the generated answer with the user query and assign a relevancy score from 1 to 5, based on how well the answer aligns with the user's intent and information need. Focus solely on whether the answer is relevant and responsive to the query, regardless of supporting context or ground truth.",
                "user": "Please evaluate the following answer for relevancy to the given query, using the rubric below:\n\n---\n\nüìå **Rubric for Answer Relevancy (1‚Äì5):**\n\n**5 - Fully Relevant:** The answer completely and directly addresses the user query, with no off-topic or irrelevant content.\n\n**4 - Mostly Relevant:** The answer is strongly related to the query and mostly fulfills its intent, with only minor off-topic details or slight undercoverage.\n\n**3 - Somewhat Relevant:** The answer has partial relevance. It touches on the topic but misses important aspects of the intent or includes moderately unrelated content.\n\n**2 - Weakly Relevant:** The answer is only loosely related to the query and fails to meet the query's intent. It contains significant irrelevant content.\n\n**1 - Not Relevant:** The answer is completely unrelated to the query.\n\n---\n\n‚úâÔ∏è **User Query:**\n{user_query}\n\nüßæ **RAG Generated Answer:**\n{generated_answer}\n\n---\n\nüîç Please return your evaluation in the following format:\n```json\n{{\n  \"score\": <1-5>,\n  \"justification\": \"<Brief explanation of your rating>\"\n}}\n```"
            },
            "contextRelevancy": {
                "system": "You are a skilled evaluator tasked with judging the relevancy of retrieved context in response to a user query. Your goal is to assess how relevant the retrieved context is to the query, on a scale from 1 to 5, based on how well it supports answering the query. Focus only on the alignment between the context and the query ‚Äî not on the final answer.",
                "user": "Evaluate the relevance of the retrieved context with respect to the given query using the following rubric:\n\n---\n\nüìå **Rubric for Context Relevancy (1‚Äì5):**\n\n**5 - Highly Relevant:** The context directly supports answering the core intent of the query with high specificity and usefulness.\n\n**4 - Mostly Relevant:** The context covers most aspects of the query, with minor gaps or generalizations.\n\n**3 - Somewhat Relevant:** The context is partially related to the query but lacks specificity or focus; helpful to some extent.\n\n**2 - Weakly Relevant:** The context contains only loose or tangential references to the query, offering little support.\n\n**1 - Not Relevant:** The context is unrelated or off-topic and does not help in answering the query.\n\n---\n\n‚úâÔ∏è **User Query:**\n{user_query}\n\nüìö **Retrieved Context:**\n{retrieved_context}\n\n---\n\nüîç Please return your evaluation in the following format:\n```json\n{{\n  \"score\": <1-5>,\n  \"justification\": \"<Brief explanation of your rating>\"\n}}\n```"
            },
            "answerCorrectness": {
                "system": "You are an expert evaluator assessing the correctness of an answer generated by a Retrieval-Augmented Generation (RAG) system. Your goal is to determine how accurately the generated answer aligns with the ground truth and how well it addresses the user's original query. You will assign a score from 1 (Completely Incorrect) to 5 (Completely Correct), using a detailed rubric.",
                "user": "Given a user query, a ground truth answer, and a RAG-generated answer, please evaluate the correctness of the generated answer using the rubric below:\n\nüéØ **Rubric for Answer Correctness**\n\n**5 - Completely Correct:** The generated answer fully matches the ground truth in meaning and detail. It accurately answers the query without introducing any errors or omissions.\n\n**4 - Mostly Correct:** The generated answer is largely accurate and aligns closely with the ground truth, with only minor omissions or slight differences in wording that do not affect the core meaning.\n\n**3 - Partially Correct:** The answer includes some correct information relevant to the query and ground truth, but misses important points or includes minor factual inaccuracies.\n\n**2 - Weakly Correct:** The answer contains fragments of relevant information but is mostly incorrect or significantly incomplete compared to the ground truth.\n\n**1 - Completely Incorrect:** The answer does not match the ground truth at all. It is factually wrong, irrelevant, or misleading with respect to the query.\n\n---\n\nüì• **Input Format**\n\n**User Query:**\n{user_query}\n\n**Ground Truth Answer:**\n{ground_truth_answer}\n\n**RAG Generated Answer:**\n{generated_answer}\n\n---\n\nüß† **Instructions**\n- Carefully read the query, ground truth, and generated answer.\n- Compare the generated answer with the ground truth, considering how well it answers the original query.\n- Assign a correctness score (1‚Äì5) based on the rubric above.\n- Provide a brief justification (1‚Äì2 sentences) for your score.\n\n---\n\nüìù **Output Format**\n```json\n{{\n  \"score\": <1-5>,\n  \"justification\": \"<Brief explanation of your rating>\"\n}}\n```"
            }
        }
    
    def _is_valid_config_value(self, value: str) -> bool:
        """Check if a config value is valid (not a placeholder)"""
        if not value or not isinstance(value, str):
            return False
        
        # Check for common placeholder patterns
        placeholders = [
            '<', '>', 'AZURE_BASE_URL', 'MODEL_DEPLOYMENT', 'EMBEDDING_DEPLOYMENT',
            'OPENAI_API_KEY', 'AZURE_OPENAI_API_KEY', 'YOUR_', 'REPLACE_'
        ]
        
        value_upper = value.upper()
        return not any(placeholder in value_upper for placeholder in placeholders)
    
    def _has_valid_azure_config(self) -> bool:
        """Check if Azure configuration has valid (non-placeholder) values"""
        return (
            self._is_valid_config_value(self.azure_config.get('api_key', '')) and
            self._is_valid_config_value(self.azure_config.get('base_url', '')) and
            self._is_valid_config_value(self.azure_config.get('model_deployment', ''))
        )
    
    def _has_valid_openai_config(self) -> bool:
        """Check if OpenAI configuration has valid (non-placeholder) values"""
        return self._is_valid_config_value(self.openai_config.get('api_key', ''))
    
    def _user_selected_openai_model(self) -> bool:
        """Check if user explicitly selected an OpenAI model in the UI"""
        # Check if any config has a model name starting with 'openai-'
        openai_model = self.openai_config.get('model_name', '')
        azure_model = self.azure_config.get('model_name', '')
        
        return (
            openai_model.startswith('openai-') or 
            azure_model.startswith('openai-') or
            openai_model.startswith('gpt-')  # Direct OpenAI model names
        )
    
    def _get_api_key(self) -> str:
        """Get API key from config or environment, prioritizing valid configurations"""
        # Priority 1: Valid OpenAI config (user provided OpenAI API key)
        if self._has_valid_openai_config():
            logger.info("üîë Using OpenAI API key from config")
            return self.openai_config['api_key']
        
        # Priority 2: User selected OpenAI model, try environment
        elif self._user_selected_openai_model() and os.getenv('OPENAI_API_KEY'):
            logger.info("üîë Using OpenAI API key from environment (user selected OpenAI model)")
            return os.getenv('OPENAI_API_KEY')
        
        # Priority 3: Valid Azure config
        elif self._has_valid_azure_config():
            logger.info("üîë Using Azure OpenAI API key from config")
            return self.azure_config['api_key']
        
        # Priority 4: Environment variables (fallback)
        elif os.getenv('OPENAI_API_KEY'):
            logger.info("üîë Using OpenAI API key from environment (fallback)")
            return os.getenv('OPENAI_API_KEY')
        elif os.getenv('AZURE_OPENAI_API_KEY'):
            logger.info("üîë Using Azure OpenAI API key from environment (fallback)")
            return os.getenv('AZURE_OPENAI_API_KEY')
        else:
            return ""
    
    def _get_model_name(self) -> str:
        """Get model name from config, prioritizing valid configurations"""
        # Priority 1: Valid OpenAI config
        if self._has_valid_openai_config() and self.openai_config.get('model_name'):
            logger.info(f"ü§ñ Using OpenAI model: {self.openai_config['model_name']}")
            return self.openai_config['model_name']
        
        # Priority 2: Valid Azure config  
        elif self._has_valid_azure_config() and self.azure_config.get('model_name'):
            logger.info(f"ü§ñ Using Azure OpenAI model: {self.azure_config['model_name']}")
            return self.azure_config['model_name']
        
        # Priority 3: Fallback from any config
        elif self.openai_config.get('model_name'):
            return self.openai_config['model_name']
        elif self.azure_config.get('model_name'):
            return self.azure_config['model_name']
        else:
            return "gpt-4o"
    
    def _get_base_url(self) -> str:
        """Get base URL for API calls, prioritizing valid configurations"""
        # Use Azure only if we have valid Azure config AND user didn't explicitly select OpenAI
        if self._has_valid_azure_config() and not self._user_selected_openai_model():
            # Azure OpenAI format
            base_url = self.azure_config['base_url'].rstrip('/')
            deployment = self.azure_config.get('model_deployment', self.model_name)
            api_version = self.azure_config.get('openai_api_version', '2024-02-15-preview')
            azure_url = f"{base_url}/openai/deployments/{deployment}/chat/completions?api-version={api_version}"
            logger.info(f"üåê Using Azure OpenAI endpoint")
            return azure_url
        else:
            # Standard OpenAI format (default, fallback, and when user selected OpenAI)
            openai_url = "https://api.openai.com/v1/chat/completions"
            logger.info(f"üåê Using OpenAI endpoint")
            return openai_url
    
    def _get_headers(self) -> Dict[str, str]:
        """Get headers for API requests, matching the chosen API provider"""
        headers = {
            "Content-Type": "application/json"
        }
        
        # Use Azure headers only if we have valid Azure config AND user didn't select OpenAI
        if self._has_valid_azure_config() and not self._user_selected_openai_model():
            headers["api-key"] = self.api_key
            logger.info("üîê Using Azure OpenAI authentication headers")
        else:
            # Use OpenAI headers (default, fallback, and when user selected OpenAI)
            headers["Authorization"] = f"Bearer {self.api_key}"
            logger.info("üîê Using OpenAI authentication headers")
            
            # Add OpenAI organization if available
            if self.openai_config.get('org_id') and self._is_valid_config_value(self.openai_config.get('org_id', '')):
                headers["OpenAI-Organization"] = self.openai_config['org_id']
                logger.info(f"üè¢ Using OpenAI organization: {self.openai_config['org_id']}")
        
        return headers
    
    def get_evaluation_prompt(self, prompt_type: str, **kwargs) -> List[Dict[str, str]]:
        """
        Get evaluation prompt based on prompt type.
        
        Args:
            prompt_type: Type of evaluation prompt ('answerRelevancy', 'contextRelevancy', 'answerCorrectness')
            **kwargs: Parameters to format the prompt template
            
        Returns:
            List of message dictionaries for the LLM API
            
        Raises:
            ValueError: If prompt_type is not supported
        """
        # Validate prompt type
        valid_types = ['answerRelevancy', 'contextRelevancy', 'answerCorrectness', 'groundTruthValidity', 'answerCompleteness']
        if prompt_type not in valid_types:
            raise ValueError(f"Invalid prompt_type: {prompt_type}. Must be one of {valid_types}")
        
        # Get prompt data from loaded prompts
        prompt_data = self.prompts.get(prompt_type, {})
        if not prompt_data:
            logger.warning(f"‚ö†Ô∏è No prompt data found for type '{prompt_type}', using default")
            return self._get_default_prompt(prompt_type, **kwargs)
        
        system_content = prompt_data.get('system', '')
        user_content_template = prompt_data.get('user', '')
        
        if not user_content_template:
            logger.warning(f"‚ö†Ô∏è No user content template found for type '{prompt_type}', using default")
            return self._get_default_prompt(prompt_type, **kwargs)
        
        try:
            # Format the user content with the provided parameters
            user_content = user_content_template.format(**kwargs)
        except KeyError as e:
            logger.error(f"‚ùå Missing required parameter for prompt type '{prompt_type}': {e}")
            return self._get_default_prompt(prompt_type, **kwargs)
        except Exception as e:
            logger.error(f"‚ùå Error formatting prompt for type '{prompt_type}': {e}")
            return self._get_default_prompt(prompt_type, **kwargs)
        
        return [
            {
                "role": "system",
                "content": system_content
            },
            {
                "role": "user",
                "content": user_content
            }
        ]
    
    def _get_default_prompt(self, prompt_type: str, **kwargs) -> List[Dict[str, str]]:
        """Get default prompt as fallback when JSON loading fails"""
        default_prompts = self._get_default_prompts()
        prompt_data = default_prompts.get(prompt_type, {})
        
        system_content = prompt_data.get('system', '')
        user_content_template = prompt_data.get('user', '')
        
        try:
            user_content = user_content_template.format(**kwargs)
        except Exception as e:
            logger.error(f"‚ùå Error formatting default prompt for type '{prompt_type}': {e}")
            return [
                {
                    "role": "system",
                    "content": "You are an evaluator. Please provide a score and justification."
                },
                {
                    "role": "user",
                    "content": f"Evaluate the provided data. Type: {prompt_type}, Data: {kwargs}"
                }
            ]
        
        return [
            {
                "role": "system",
                "content": system_content
            },
            {
                "role": "user",
                "content": user_content
            }
        ]
    

    
    async def call_openai_api(self, session: aiohttp.ClientSession, messages: List[Dict[str, str]]) -> Optional[Dict[str, Any]]:
        """Make an API call to OpenAI/Azure OpenAI and capture token usage"""
        try:
            payload = {
                "messages": messages,
                "model": self.model_name,
                "temperature": 0.1,
                "max_tokens": 500,
                "response_format": {"type": "json_object"}
            }
            
            async with session.post(
                self.base_url,
                headers=self.headers,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    content = result.get('choices', [{}])[0].get('message', {}).get('content', '{}')
                    
                    # Extract token usage information
                    usage = result.get('usage', {})
                    token_usage = {
                        'prompt_tokens': usage.get('prompt_tokens', 0),
                        'completion_tokens': usage.get('completion_tokens', 0),
                        'total_tokens': usage.get('total_tokens', 0)
                    }
                    
                    # Parse the JSON response
                    try:
                        evaluation = json.loads(content)
                        # Include token usage in the response
                        evaluation['token_usage'] = token_usage
                        return evaluation
                    except json.JSONDecodeError:
                        logger.error(f"Failed to parse JSON response: {content}")
                        return {
                            "score": 3, 
                            "justification": "Failed to parse LLM response",
                            "token_usage": token_usage
                        }
                else:
                    error_text = await response.text()
                    logger.error(f"API request failed with status {response.status}: {error_text}")
                    return None
        
        except Exception as e:
            logger.error(f"Error calling OpenAI API: {e}")
            return None
    
    async def evaluate_answer_relevancy(self, session: aiohttp.ClientSession, query: str, answer: str) -> Dict[str, Any]:
        """Evaluate answer relevancy using LLM"""
        try:
            messages = self.get_evaluation_prompt('answerRelevancy', user_query=query, generated_answer=answer)
            result = await self.call_openai_api(session, messages)
            
            if result and 'score' in result:
                # Convert 1-5 scale to 0-1 scale for consistency with RAGAS
                score = float(result['score']) / 5.0
                justification = result.get('justification', 'No justification provided')
                logger.info(f"üéØ Answer Relevancy: {score:.3f} - {justification}")
                return {'score': score, 'justification': justification}
            else:
                logger.warning("Failed to get answer relevancy score, using default")
                return {'score': 0.5, 'justification': 'Failed to get evaluation from LLM'}
        
        except Exception as e:
            logger.error(f"Error evaluating answer relevancy: {e}")
            return {'score': 0.5, 'justification': f'Error during evaluation: {str(e)}'}
    
    async def evaluate_context_relevancy(self, session: aiohttp.ClientSession, query: str, context: str) -> Dict[str, Any]:
        """Evaluate context relevancy using LLM"""
        try:
            messages = self.get_evaluation_prompt('contextRelevancy', user_query=query, retrieved_context=context)
            result = await self.call_openai_api(session, messages)
            
            if result and 'score' in result:
                # Convert 1-5 scale to 0-1 scale for consistency with RAGAS
                score = float(result['score']) / 5.0
                justification = result.get('justification', 'No justification provided')
                logger.info(f"üéØ Context Relevancy: {score:.3f} - {justification}")
                return {'score': score, 'justification': justification}
            else:
                logger.warning("Failed to get context relevancy score, using default")
                return {'score': 0.5, 'justification': 'Failed to get evaluation from LLM'}
        
        except Exception as e:
            logger.error(f"Error evaluating context relevancy: {e}")
            return {'score': 0.5, 'justification': f'Error during evaluation: {str(e)}'}
    
    async def evaluate_answer_correctness(self, session: aiohttp.ClientSession, query: str, ground_truth: str, answer: str) -> Dict[str, Any]:
        """Evaluate answer correctness using LLM"""
        try:
            messages = self.get_evaluation_prompt('answerCorrectness', user_query=query, ground_truth_answer=ground_truth, generated_answer=answer)
            result = await self.call_openai_api(session, messages)
            
            if result and 'score' in result:
                # Convert 1-5 scale to 0-1 scale for consistency with RAGAS
                score = float(result['score']) / 5.0
                justification = result.get('justification', 'No justification provided')
                logger.info(f"üéØ Answer Correctness: {score:.3f} - {justification}")
                return {'score': score, 'justification': justification}
            else:
                logger.warning("Failed to get answer correctness score, using default")
                return {'score': 0.5, 'justification': 'Failed to get evaluation from LLM'}
        
        except Exception as e:
            logger.error(f"Error evaluating answer correctness: {e}")
            return {'score': 0.5, 'justification': f'Error during evaluation: {str(e)}'}
    
    async def evaluate_ground_truth_validity(self, session: aiohttp.ClientSession, query: str, ground_truth: str) -> Dict[str, Any]:
        """Evaluate ground truth validity using LLM"""
        try:
            messages = self.get_evaluation_prompt('groundTruthValidity', user_query=query, ground_truth_answer=ground_truth)
            result = await self.call_openai_api(session, messages)
            
            if result and 'score' in result:
                # Convert 1-5 scale to 0-1 scale for consistency with RAGAS
                score = float(result['score']) / 5.0
                justification = result.get('justification', 'No justification provided')
                logger.info(f"üéØ Ground Truth Validity: {score:.3f} - {justification}")
                return {'score': score, 'justification': justification}
            else:
                logger.warning("Failed to get ground truth validity score, using default")
                return {'score': 0.5, 'justification': 'Failed to get evaluation from LLM'}
        
        except Exception as e:
            logger.error(f"Error evaluating ground truth validity: {e}")
            return {'score': 0.5, 'justification': f'Error during evaluation: {str(e)}'}
    
    async def evaluate_answer_completeness(self, session: aiohttp.ClientSession, query: str, answer: str) -> Dict[str, Any]:
        """Evaluate answer completeness using LLM"""
        try:
            messages = self.get_evaluation_prompt('answerCompleteness', user_query=query, generated_answer=answer)
            result = await self.call_openai_api(session, messages)
            
            if result and 'score' in result:
                # Convert 1-5 scale to 0-1 scale for consistency with RAGAS
                score = float(result['score']) / 5.0
                justification = result.get('justification', 'No justification provided')
                logger.info(f"üéØ Answer Completeness: {score:.3f} - {justification}")
                return {'score': score, 'justification': justification}
            else:
                logger.warning("Failed to get answer completeness score, using default")
                return {'score': 0.5, 'justification': 'Failed to get evaluation from LLM'}
        
        except Exception as e:
            logger.error(f"Error evaluating answer completeness: {e}")
            return {'score': 0.5, 'justification': f'Error during evaluation: {str(e)}'}
    
    async def evaluate_batch(self, session: aiohttp.ClientSession, data_batch: List[Dict[str, Any]], 
                           batch_size: int = 5) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """Evaluate a batch of data using LLM metrics with concurrent processing"""
        logger.info(f"ü§ñ Starting LLM evaluation for batch of {len(data_batch)} items")
        
        # Initialize token usage tracking
        total_token_usage = {
            'prompt_tokens': 0,
            'completion_tokens': 0,
            'total_tokens': 0
        }
        
        # Process items in smaller concurrent batches for better performance
        semaphore = asyncio.Semaphore(batch_size)
        
        async def evaluate_single_item(item: Dict[str, Any]) -> Dict[str, Any]:
            async with semaphore:
                try:
                    query = item.get('query', '')
                    answer = item.get('answer', '')
                    ground_truth = item.get('ground_truth', '')
                    context = item.get('context', [])
                    
                    # Convert context to string if it's a list
                    context_str = ' '.join(context) if isinstance(context, list) else str(context)
                    
                    # Initialize result with core input data to ensure it's always present in output
                    result = {
                        'query': query,
                        'answer': answer,
                        'ground_truth': ground_truth,
                        'context': context_str
                    }
                    
                    # Evaluate each metric concurrently
                    tasks = []
                    
                    if answer and query:
                        tasks.append(self.evaluate_answer_relevancy(session, query, answer))
                    else:
                        tasks.append(asyncio.create_task(self._return_default_result(0.5, 'No answer or query provided')))
                    
                    if context_str and query:
                        tasks.append(self.evaluate_context_relevancy(session, query, context_str))
                    else:
                        tasks.append(asyncio.create_task(self._return_default_result(0.5, 'No context or query provided')))
                    
                    if answer and ground_truth and query:
                        tasks.append(self.evaluate_answer_correctness(session, query, ground_truth, answer))
                    else:
                        tasks.append(asyncio.create_task(self._return_default_result(0.5, 'No answer, ground truth, or query provided')))
                    
                    # New metrics: Ground Truth Validity and Answer Completeness
                    if ground_truth and query:
                        tasks.append(self.evaluate_ground_truth_validity(session, query, ground_truth))
                    else:
                        tasks.append(asyncio.create_task(self._return_default_result(0.5, 'No ground truth or query provided')))
                    
                    if answer and query:
                        tasks.append(self.evaluate_answer_completeness(session, query, answer))
                    else:
                        tasks.append(asyncio.create_task(self._return_default_result(0.5, 'No answer or query provided')))
                    
                    # Execute all evaluations concurrently for this item
                    evaluation_results = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    # Handle results and extract scores, justifications, and token usage
                    default_result = {'score': 0.5, 'justification': 'Error during evaluation', 'token_usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}}
                    
                    answer_relevancy_result = evaluation_results[0] if not isinstance(evaluation_results[0], Exception) else default_result
                    context_relevancy_result = evaluation_results[1] if not isinstance(evaluation_results[1], Exception) else default_result
                    answer_correctness_result = evaluation_results[2] if not isinstance(evaluation_results[2], Exception) else default_result
                    ground_truth_validity_result = evaluation_results[3] if not isinstance(evaluation_results[3], Exception) else default_result
                    answer_completeness_result = evaluation_results[4] if not isinstance(evaluation_results[4], Exception) else default_result
                    
                    # Aggregate token usage from all evaluations for this item
                    item_token_usage = {
                        'prompt_tokens': 0,
                        'completion_tokens': 0,
                        'total_tokens': 0
                    }
                    
                    for eval_result in [answer_relevancy_result, context_relevancy_result, answer_correctness_result, ground_truth_validity_result, answer_completeness_result]:
                        if 'token_usage' in eval_result:
                            token_data = eval_result['token_usage']
                            item_token_usage['prompt_tokens'] += token_data.get('prompt_tokens', 0)
                            item_token_usage['completion_tokens'] += token_data.get('completion_tokens', 0)
                            item_token_usage['total_tokens'] += token_data.get('total_tokens', 0)
                    
                    # Add to global token usage tracking (thread-safe update needed)
                    nonlocal total_token_usage
                    total_token_usage['prompt_tokens'] += item_token_usage['prompt_tokens']
                    total_token_usage['completion_tokens'] += item_token_usage['completion_tokens']
                    total_token_usage['total_tokens'] += item_token_usage['total_tokens']
                    
                    # Add LLM metrics to result with both scores and justifications
                    result.update({
                        'LLM Answer Relevancy': answer_relevancy_result.get('score', 0.5),
                        'LLM Answer Relevancy Justification': answer_relevancy_result.get('justification', 'No justification available'),
                        'LLM Context Relevancy': context_relevancy_result.get('score', 0.5),
                        'LLM Context Relevancy Justification': context_relevancy_result.get('justification', 'No justification available'),
                        'LLM Answer Correctness': answer_correctness_result.get('score', 0.5),
                        'LLM Answer Correctness Justification': answer_correctness_result.get('justification', 'No justification available'),
                        'LLM Ground Truth Validity': ground_truth_validity_result.get('score', 0.5),
                        'LLM Ground Truth Validity Justification': ground_truth_validity_result.get('justification', 'No justification available'),
                        'LLM Answer Completeness': answer_completeness_result.get('score', 0.5),
                        'LLM Answer Completeness Justification': answer_completeness_result.get('justification', 'No justification available')
                    })
                    
                    return result
                    
                except Exception as e:
                    logger.error(f"Error evaluating item: {e}")
                    # Return item with default scores and justifications
                    return {
                        'query': item.get('query', ''),
                        'answer': item.get('answer', ''),
                        'ground_truth': item.get('ground_truth', ''),
                        'context': item.get('context', []),
                        'LLM Answer Relevancy': 0.5,
                        'LLM Answer Relevancy Justification': f'Error during evaluation: {str(e)}',
                        'LLM Context Relevancy': 0.5,
                        'LLM Context Relevancy Justification': f'Error during evaluation: {str(e)}',
                        'LLM Answer Correctness': 0.5,
                        'LLM Answer Correctness Justification': f'Error during evaluation: {str(e)}',
                        'LLM Ground Truth Validity': 0.5,
                        'LLM Ground Truth Validity Justification': f'Error during evaluation: {str(e)}',
                        'LLM Answer Completeness': 0.5,
                        'LLM Answer Completeness Justification': f'Error during evaluation: {str(e)}'
                    }
        
        # Process all items concurrently with controlled concurrency
        logger.info(f"üîÑ Processing {len(data_batch)} items with max {batch_size} concurrent evaluations")
        start_time = time.time()
        
        results = await asyncio.gather(*[evaluate_single_item(item) for item in data_batch], 
                                     return_exceptions=True)
        
        # Handle any exceptions in results
        final_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Exception in item evaluation: {result}")
                final_results.append({
                    'query': '', 'answer': '', 'ground_truth': '', 'context': [],
                    'LLM Answer Relevancy': 0.5,
                    'LLM Answer Relevancy Justification': f'Exception during evaluation: {str(result)}',
                    'LLM Context Relevancy': 0.5,
                    'LLM Context Relevancy Justification': f'Exception during evaluation: {str(result)}',
                    'LLM Answer Correctness': 0.5,
                    'LLM Answer Correctness Justification': f'Exception during evaluation: {str(result)}',
                    'LLM Ground Truth Validity': 0.5,
                    'LLM Ground Truth Validity Justification': f'Exception during evaluation: {str(result)}',
                    'LLM Answer Completeness': 0.5,
                    'LLM Answer Completeness Justification': f'Exception during evaluation: {str(result)}'
                })
            else:
                final_results.append(result)
        
        eval_time = time.time() - start_time
        logger.info(f"‚úÖ Completed LLM evaluation for {len(final_results)} items in {eval_time:.2f}s")
        logger.info(f"üí∞ LLM Token usage: {total_token_usage}")
        
        return final_results, total_token_usage

    async def _return_default_result(self, score: float, justification: str) -> Dict[str, Any]:
        """Helper function to return a default result with score and justification asynchronously"""
        await asyncio.sleep(0)
        return {
            'score': score, 
            'justification': justification,
            'token_usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}
        }
    
    def get_average_scores(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """Calculate average scores for LLM metrics"""
        if not results:
            return {}
        
        llm_metrics = ['LLM Answer Relevancy', 'LLM Context Relevancy', 'LLM Answer Correctness', 'LLM Ground Truth Validity', 'LLM Answer Completeness']
        averages = {}
        
        for metric in llm_metrics:
            scores = [r.get(metric, 0) for r in results if isinstance(r.get(metric), (int, float))]
            if scores:
                averages[metric] = sum(scores) / len(scores)
                logger.info(f"üìä Average {metric}: {averages[metric]:.4f}")
            else:
                averages[metric] = 0.0
        
        return averages

    # Required abstract methods from BaseEvaluator
    async def evaluate(self, queries: List[str], answers: List[str], ground_truths: List[str], contexts: List[str]) -> tuple:
        """
        Implement the abstract evaluate method from BaseEvaluator.
        This method provides compatibility with the base class interface.
        """
        try:
            logger.info(f"ü§ñ Starting LLM evaluation for {len(queries)} queries")
            
            # Prepare data for evaluation
            eval_data = []
            for i in range(len(queries)):
                eval_data.append({
                    'query': queries[i] if i < len(queries) else '',
                    'answer': answers[i] if i < len(answers) else '',
                    'ground_truth': ground_truths[i] if i < len(ground_truths) else '',
                    'context': contexts[i] if i < len(contexts) else []
                })
            
            # Run evaluation with async session
            async with aiohttp.ClientSession() as session:
                results_list, total_token_usage = await self.evaluate_batch(session, eval_data)
            
            # Convert to DataFrame and include token usage in metadata
            if results_list:
                results_df = pd.DataFrame(results_list)
                averages = self.get_average_scores(results_list)
                
                # Calculate estimated cost using real model pricing
                if total_token_usage and total_token_usage.get('total_tokens', 0) > 0:
                    model_name = self.model_name.lower()
                    
                    # Model pricing per 1K tokens (input, output)
                    model_pricing = {
                        'gpt-4': {'input': 0.03, 'output': 0.06},
                        'gpt-4-turbo': {'input': 0.01, 'output': 0.03},
                        'gpt-4o': {'input': 0.005, 'output': 0.015},
                        'gpt-4o-mini': {'input': 0.00015, 'output': 0.0006},
                        'gpt-3.5-turbo': {'input': 0.0005, 'output': 0.0015},
                        'claude-3-opus': {'input': 0.015, 'output': 0.075},
                        'claude-3-sonnet': {'input': 0.003, 'output': 0.015},
                        'claude-3-haiku': {'input': 0.00025, 'output': 0.00125},
                        'claude-3.5-sonnet': {'input': 0.003, 'output': 0.015}
                    }
                    
                    # Find matching pricing
                    pricing = None
                    for model_key, model_prices in model_pricing.items():
                        if model_key in model_name or model_name in model_key:
                            pricing = model_prices
                            break
                    
                    # Default to GPT-4o pricing if model not found
                    if not pricing:
                        logger.warning(f"‚ö†Ô∏è Unknown model '{model_name}', using GPT-4o pricing")
                        pricing = model_pricing['gpt-4o']
                    
                    # Calculate cost
                    input_cost = (total_token_usage.get('prompt_tokens', 0) / 1000) * pricing['input']
                    output_cost = (total_token_usage.get('completion_tokens', 0) / 1000) * pricing['output']
                    estimated_cost = input_cost + output_cost
                    
                    total_token_usage['estimated_cost_usd'] = estimated_cost
                    logger.info(f"üí∞ LLM cost calculation: Input=${input_cost:.6f}, Output=${output_cost:.6f}, Total=${estimated_cost:.6f}")
                
                # Create enhanced metadata with token usage
                enhanced_metadata = {
                    **averages,
                    'token_usage': total_token_usage,
                    'evaluation_summary': {
                        'total_queries': len(results_list),
                        'model_used': self.model_name,
                        'evaluator_type': 'LLM'
                    }
                }
                
                logger.info(f"‚úÖ LLM evaluation completed: {len(results_df)} rows")
                return results_df, enhanced_metadata
            else:
                logger.warning("‚ö†Ô∏è No LLM results generated")
                return pd.DataFrame(), {}
                
        except Exception as e:
            logger.error(f"‚ùå LLM evaluation failed: {e}")
            return pd.DataFrame(), {}

    def process_results(self, results) -> Dict[str, Any]:
        """
        Implement the abstract process_results method from BaseEvaluator.
        This method processes evaluation results and returns summary statistics.
        """
        try:
            if hasattr(results, 'to_dict'):
                # If results is a DataFrame, convert to dict
                results_dict = results.to_dict('records')
            elif isinstance(results, list):
                results_dict = results
            else:
                results_dict = []
            
            # Calculate averages and summary statistics
            averages = self.get_average_scores(results_dict)
            
            summary = {
                'total_queries': len(results_dict),
                'averages': averages,
                'evaluator_type': 'LLM',
                'metrics_included': ['LLM Answer Relevancy', 'LLM Context Relevancy', 'LLM Answer Correctness']
            }
            
            logger.info(f"üìä LLM evaluation summary: {summary}")
            return summary
            
        except Exception as e:
            logger.error(f"‚ùå Error processing LLM results: {e}")
            return {
                'total_queries': 0,
                'averages': {},
                'evaluator_type': 'LLM',
                'metrics_included': [],
                'error': str(e)
            } 