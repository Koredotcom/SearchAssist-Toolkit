# üöÄ RAG Evaluator Web UI - User Guide

## ‚ú® Overview

The RAG Evaluator Web UI provides a modern, user-friendly interface for evaluating RAG (Retrieval-Augmented Generation) systems using RAGAS and CRAG metrics. This enhanced version features **asynchronous batch processing** for 3-5x performance improvement and beautiful visualizations.

![RAG Evaluator UI](https://img.shields.io/badge/Version-v2.0-blue) ![Async Processing](https://img.shields.io/badge/Async-Batch%20Processing-green) ![UI](https://img.shields.io/badge/UI-Modern%20Web-purple)

## üèÉ‚Äç‚ôÇÔ∏è Quick Start

### 1. Start the UI Server

```bash
# Option 1: Using the startup script (recommended)
python start_ui.py

# Option 2: Direct FastAPI start
cd src && python routes/app.py
```

### 2. Access the Interface

- **Main UI**: http://localhost:8001
- **API Docs**: http://localhost:8001/api/docs
- **Health Check**: http://localhost:8001/api/health

## üìã Step-by-Step Usage Guide

### Step 1: Upload Your Excel File üìÇ

![Upload File](docs/upload-demo.gif)

1. **Drag & Drop**: Simply drag your Excel file (`.xlsx` or `.xls`) onto the upload area
2. **Browse**: Click the upload area to browse and select files
3. **Validation**: The system automatically validates file format and size
4. **Sheet Detection**: Available sheets are automatically detected and listed

**Required Excel Structure:**
```
| query          | ground_truth           | contexts (optional) | answer (optional) |
|----------------|------------------------|-------------------|------------------|
| What is AI?    | AI is artificial...    | ["context1", ...] | AI stands for... |
| How does ML... | Machine learning...    | ["context2", ...] | Machine learning.|
```

### Step 2: Configure Evaluation Settings ‚öôÔ∏è

![Configuration](docs/config-demo.gif)

#### Evaluation Methods
- **‚úÖ RAGAS Evaluation**: Response relevancy, faithfulness, context recall, context precision, answer correctness, semantic similarity
- **‚úÖ CRAG Evaluation**: Accuracy assessment using LLM judgment
- **‚úÖ LLM Evaluation**: Custom OpenAI/Azure-based correctness and relevancy assessment
- **üîç Use Search API**: Fetch responses from SearchAssist/XO API
- **üíæ Save to Database**: Store results in MongoDB

#### Performance Settings
Choose from preset configurations or customize:

| Preset | Batch Size | Max Concurrent | Best For |
|--------|------------|----------------|----------|
| **üê¢ Conservative** | 5 | 2 | Rate-limited APIs, small datasets |
| **‚öñÔ∏è Balanced** | 10 | 5 | Most use cases (recommended) |
| **üöÄ High Performance** | 20 | 10 | Powerful systems, unrestricted APIs |

#### Advanced Options
- **ü§ñ LLM Model**: Choose from GPT-4o, GPT-4o Mini (OpenAI or Azure)
- **üìä Sheet Selection**: Process specific sheets or all sheets  
- **üìß Email Reports**: Automatically send results via email
- **üîê API Configuration**: Secure input for SearchAssist/XO and LLM credentials

### Step 3: Run Evaluation ‚ñ∂Ô∏è

![Processing](docs/processing-demo.gif)

1. **Validation**: System validates all settings and file structure
2. **Estimation**: Shows estimated processing time and query count
3. **Execution**: Real-time progress tracking with detailed logs
4. **Monitoring**: Track completed queries, elapsed time, and remaining time

**Performance Comparison:**
- **Traditional (Sequential)**: 100 queries = ~5-8 minutes
- **New (Async Batch)**: 100 queries = ~1-2 minutes ‚ö°

### Step 4: View Results üìä

![Results](docs/results-demo.gif)

#### Summary Dashboard
- **üìà Total Processed**: Number of queries evaluated
- **‚úÖ Success Rate**: Percentage of successful evaluations  
- **‚è±Ô∏è Performance Metrics**: Total time, average per query
- **üìâ Evaluation Scores**: Visual radar chart of RAGAS/CRAG metrics

#### Available Actions
- **üíæ Download Results**: Get Excel file with detailed results
- **üëÅÔ∏è View Details**: Explore individual query results
- **üîÑ New Evaluation**: Start fresh evaluation

#### Output Structure
```
your_file_evaluation_output_20240315-143022.xlsx
‚îú‚îÄ‚îÄ Sheet1                 (Main evaluation results)
‚îú‚îÄ‚îÄ Sheet2                 (Additional sheet results)
‚îú‚îÄ‚îÄ Processing_Summary     (Success/failure statistics)
‚îú‚îÄ‚îÄ Processing_Metadata    (Configuration and timing)
‚îî‚îÄ‚îÄ ERROR_Sheet3          (Detailed error information)
```

## üéØ Features & Benefits

### üöÄ Performance Improvements
- **3-5x Faster Processing**: Asynchronous batch processing
- **Smart Rate Limiting**: Automatic API throttling
- **Memory Efficient**: Processes large datasets without memory issues
- **Error Isolation**: Failed queries don't stop entire process

### üé® User Experience
- **Modern Interface**: Clean, intuitive design
- **Real-time Progress**: Live updates with detailed logs
- **Drag & Drop Upload**: Seamless file handling
- **Mobile Responsive**: Works on all device sizes
- **Toast Notifications**: Instant feedback for all actions

### üìä Advanced Analytics
- **Radar Chart Visualization**: Beautiful RAGAS metrics display
- **Processing Statistics**: Detailed performance analytics
- **Error Reporting**: Comprehensive error tracking
- **Export Options**: Multiple download formats

### üõ°Ô∏è Reliability Features
- **Data Validation**: Input validation at every step
- **Error Recovery**: Graceful handling of failures
- **Backup Creation**: Automatic file backups
- **Fallback Options**: CSV export if Excel fails

## üîß Configuration Options

### LLM Evaluation Configuration

The **LLM Evaluator** provides custom evaluation using OpenAI or Azure OpenAI models with three key metrics:

#### Supported Metrics
- **Answer Correctness**: Evaluates factual accuracy and semantic correctness
- **Answer Relevancy**: Assesses how well the answer addresses the question
- **Context Relevancy**: Measures how well the context supports the answer

#### Configuration Examples

**OpenAI Configuration:**
```json
{
  "openai": {
    "api_key": "sk-your-api-key",
    "org_id": "org-your-organization", 
    "model_name": "gpt-4o",
    "embedding_name": "text-embedding-ada-002"
  }
}
```

**Azure OpenAI Configuration:**
```json
{
  "azure": {
    "api_key": "your-azure-key",
    "base_url": "https://your-resource.openai.azure.com/",
    "openai_api_version": "2024-02-15-preview",
    "model_name": "gpt-4o",
    "model_deployment": "gpt-4o-deployment",
    "embedding_name": "text-embedding-ada-002",
    "embedding_deployment": "embedding-deployment"
  }
}
```

### Search API Configuration

The system supports multiple search API integrations:

**SearchAssist (SA) Configuration:**
```json
{
  "SA": {
    "app_id": "your-searchassist-app-id",
    "client_id": "your-client-id", 
    "client_secret": "your-client-secret",
    "domain": "your-domain.com"
  }
}
```

**XO Platform (UXO) Configuration:**
```json
{
  "UXO": {
    "app_id": "your-uxo-app-id",
    "client_id": "your-client-id",
    "client_secret": "your-client-secret", 
    "domain": "your-domain.com"
  }
}
```

### Database Configuration

**MongoDB Setup (Optional):**
```json
{
  "MongoDB": {
    "url": "mongodb://localhost:27017",
    "dbName": "rag_evaluator",
    "collectionName": "evaluations"
  }
}
```

### Performance Tuning

#### For Small Datasets (‚â§50 queries)
```bash
Batch Size: 5
Max Concurrent: 2
Expected Time: 30-60 seconds
```

#### For Medium Datasets (50-200 queries)  
```bash
Batch Size: 10
Max Concurrent: 5
Expected Time: 1-3 minutes
```

#### For Large Datasets (200+ queries)
```bash
Batch Size: 20
Max Concurrent: 10
Expected Time: 3-10 minutes
```

## üîó Integration Examples

### 1. **CI/CD Pipeline Integration**

Integrate RAG evaluation into your continuous integration pipeline:

```yaml
# .github/workflows/rag-evaluation.yml
name: RAG System Evaluation

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  evaluate-rag:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'
    
    - name: Install dependencies
      run: |
        pip install requests pandas
    
    - name: Run RAG Evaluation
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        python scripts/ci_evaluation.py
    
    - name: Upload Results
      uses: actions/upload-artifact@v2
      with:
        name: rag-evaluation-results
        path: evaluation_results.xlsx
```

### 2. **Python Integration Script**

```python
# scripts/automated_evaluation.py
import requests
import json
import time
import os
from pathlib import Path

class RAGEvaluator:
    def __init__(self, base_url="http://localhost:8001"):
        self.base_url = base_url
        self.session_id = self._create_session()
    
    def _create_session(self):
        response = requests.post(f"{self.base_url}/api/create-session")
        return response.json()['session_id']
    
    def evaluate_dataset(self, file_path, config):
        """Evaluate a dataset file with given configuration"""
        
        # Upload and evaluate
        with open(file_path, 'rb') as f:
            files = {'excel_file': f}
            data = {
                'params': json.dumps(config['params']),
                'config': json.dumps(config['api_config']),
                'session_id': self.session_id
            }
            
            response = requests.post(
                f"{self.base_url}/api/runeval", 
                files=files, 
                data=data
            )
            
            if response.status_code == 200:
                return self._download_results()
            else:
                raise Exception(f"Evaluation failed: {response.text}")
    
    def _download_results(self):
        """Download evaluation results"""
        response = requests.get(
            f"{self.base_url}/api/download-results/{self.session_id}"
        )
        
        if response.status_code == 200:
            timestamp = int(time.time())
            filename = f"evaluation_results_{timestamp}.xlsx"
            
            with open(filename, 'wb') as f:
                f.write(response.content)
            
            return filename
        else:
            raise Exception(f"Download failed: {response.text}")

# Usage example
if __name__ == "__main__":
    evaluator = RAGEvaluator()
    
    config = {
        "params": {
            "sheet_name": "Sheet1",
            "evaluate_ragas": True,
            "evaluate_crag": True,
            "batch_size": 10,
            "max_concurrent": 5
        },
        "api_config": {
            "openai": {
                "api_key": os.getenv("OPENAI_API_KEY"),
                "model_name": "gpt-4o"
            }
        }
    }
    
    result_file = evaluator.evaluate_dataset("test_data.xlsx", config)
    print(f"Evaluation completed: {result_file}")
```

### 3. **Slack Bot Integration**

```python
# slack_bot_evaluation.py
from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler
import requests
import json

app = App(token=os.environ.get("SLACK_BOT_TOKEN"))

@app.command("/evaluate-rag")
def handle_evaluation_command(ack, respond, command):
    ack()
    
    # Start evaluation
    evaluator = RAGEvaluator()
    
    try:
        # Get file from Slack (implementation depends on your setup)
        file_path = download_file_from_slack(command['file_id'])
        
        config = {
            "params": {"evaluate_ragas": True, "evaluate_crag": True},
            "api_config": {"openai": {"api_key": os.getenv("OPENAI_API_KEY")}}
        }
        
        result_file = evaluator.evaluate_dataset(file_path, config)
        
        respond(f"‚úÖ RAG evaluation completed! Results: {result_file}")
        
    except Exception as e:
        respond(f"‚ùå Evaluation failed: {str(e)}")

if __name__ == "__main__":
    SocketModeHandler(app, os.environ["SLACK_APP_TOKEN"]).start()
```

### 4. **Jupyter Notebook Integration**

```python
# RAG_Evaluation_Notebook.ipynb
import requests
import json
import pandas as pd
import matplotlib.pyplot as plt

# Cell 1: Setup
evaluator = RAGEvaluator("http://localhost:8001")

# Cell 2: Configuration
config = {
    "params": {
        "sheet_name": "Sheet1",
        "evaluate_ragas": True,
        "evaluate_crag": True,
        "evaluate_llm": True
    },
    "api_config": {
        "openai": {"api_key": "your-key", "model_name": "gpt-4o"}
    }
}

# Cell 3: Run Evaluation
result_file = evaluator.evaluate_dataset("data.xlsx", config)
results_df = pd.read_excel(result_file, sheet_name="Main_Results")

# Cell 4: Visualize Results
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

# RAGAS metrics
ragas_metrics = ['response_relevancy', 'faithfulness', 'context_recall', 'context_precision']
results_df[ragas_metrics].boxplot(ax=axes[0,0])
axes[0,0].set_title('RAGAS Metrics Distribution')

# CRAG scores
results_df['score'].hist(ax=axes[0,1], bins=20)
axes[0,1].set_title('CRAG Score Distribution')

# Processing time analysis
results_df['processing_time'].plot(ax=axes[1,0])
axes[1,0].set_title('Processing Time per Query')

# Success rate
success_rate = results_df['success'].mean()
axes[1,1].pie([success_rate, 1-success_rate], labels=['Success', 'Failed'], autopct='%1.1f%%')
axes[1,1].set_title('Overall Success Rate')

plt.tight_layout()
plt.show()
```

## üö® Troubleshooting

### UI-Specific Issues

#### 1. **Browser Compatibility**
```
‚ùå Error: Interface not loading properly
‚úÖ Solution: Use modern browsers (Chrome 90+, Firefox 88+, Safari 14+)
‚úÖ Solution: Enable JavaScript and disable ad blockers
‚úÖ Solution: Clear browser cache and cookies
```

#### 2. **File Upload Issues**
```
‚ùå Error: Drag & drop not working
‚úÖ Solution: Check browser security settings
‚úÖ Solution: Ensure file size < 100MB
‚úÖ Solution: Use browse button as alternative
```

#### 3. **Session Management Problems**
```
‚ùå Error: Session expired during evaluation
‚úÖ Solution: Refresh page and restart evaluation
‚úÖ Solution: Check browser localStorage permissions
‚úÖ Solution: Use incognito mode to test
```

#### 4. **Real-time Updates Not Working**
```
‚ùå Error: Progress bar stuck at 0%
‚úÖ Solution: Check WebSocket connections (F12 ‚Üí Network)
‚úÖ Solution: Disable firewall/proxy restrictions
‚úÖ Solution: Use HTTP instead of HTTPS for local testing
```

### API and Processing Issues

#### 1. **File Upload Fails**
```
‚ùå Error: File type not supported
‚úÖ Solution: Use .xlsx or .xls files only
‚úÖ Solution: Ensure file has required columns (query, ground_truth)
‚úÖ Solution: Remove empty rows and special characters
```

#### 2. **Processing Stuck**
```  
‚ùå Error: Evaluation hangs at X%
‚úÖ Solution: Check API credentials and rate limits
‚úÖ Solution: Reduce batch size and max concurrent requests
‚úÖ Solution: Check internet connectivity
```

#### 3. **High Memory Usage**
```
‚ùå Error: Out of memory / Browser crashes
‚úÖ Solution: Reduce batch_size and max_concurrent
‚úÖ Solution: Close other browser tabs
‚úÖ Solution: Use Conservative performance preset
```

#### 4. **API Rate Limits**
```
‚ùå Error: Too many requests / 429 errors
‚úÖ Solution: Use Conservative preset or reduce concurrency
‚úÖ Solution: Add delays between batches
‚úÖ Solution: Check API quota limits
```

### Configuration Issues

#### 5. **Invalid API Keys**
```
‚ùå Error: Authentication failed
‚úÖ Solution: Verify API keys are correct and active
‚úÖ Solution: Check API key permissions and quotas
‚úÖ Solution: Use environment variables for security
```

#### 6. **Search API Configuration**
```
‚ùå Error: Search API calls failing
‚úÖ Solution: Verify domain format (no http:// prefix)
‚úÖ Solution: Check client credentials and app_id
‚úÖ Solution: Test API endpoints manually
```

### Performance Issues

#### 7. **Slow Processing**
```
‚ùå Error: Very slow evaluation speed
‚úÖ Solution: Use High Performance preset
‚úÖ Solution: Increase batch size (if API allows)
‚úÖ Solution: Check network speed and latency
‚úÖ Solution: Use local models if available
```

#### 8. **Download Issues**
```
‚ùå Error: Can't download results
‚úÖ Solution: Check popup blockers
‚úÖ Solution: Verify session is still active
‚úÖ Solution: Try alternative download method (API)
```

### Performance Optimization

#### For Rate-Limited APIs
- Use **Conservative** preset
- Increase delays between batches
- Monitor API response times

#### For High-Volume Processing
- Use **High Performance** preset
- Ensure stable internet connection
- Monitor system resources

#### For Mixed Workloads
- Use **Balanced** preset (default)
- Monitor progress and adjust if needed
- Consider processing during off-peak hours

## üèóÔ∏è Architecture Overview

### System Components

```mermaid
graph TB
    A[Web UI] --> B[FastAPI Backend]
    B --> C[Session Manager]
    B --> D[Evaluation Engine]
    D --> E[RAGAS Evaluator]
    D --> F[CRAG Evaluator]
    D --> G[LLM Evaluator]
    D --> H[Search API Layer]
    H --> I[SearchAssist API]
    H --> J[XO Platform API]
    B --> K[File Manager]
    B --> L[MongoDB]
    B --> M[Email Service]
```

### Key Design Principles

- **Session Isolation**: Each user session is completely isolated
- **Async Processing**: All evaluations run asynchronously for better performance
- **Fault Tolerance**: Robust error handling and recovery mechanisms
- **Scalability**: Designed to handle multiple concurrent users
- **Security**: Secure file handling and API key management

### Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Frontend** | HTML5, CSS3, JavaScript | Modern web interface |
| **Backend** | FastAPI, Python 3.8+ | REST API and business logic |
| **Evaluation** | RAGAS, OpenAI, Custom LLMs | RAG system assessment |
| **Database** | MongoDB (optional) | Result persistence |
| **File Processing** | Pandas, OpenPyXL | Excel file handling |
| **Async Processing** | AsyncIO, AioHTTP | Concurrent request handling |

## üìà Advanced Features

### Multi-Modal Evaluation Support

The system supports various evaluation combinations:

| Combination | Use Case | Benefits |
|-------------|----------|----------|
| **RAGAS Only** | Quick standard evaluation | Fast, comprehensive metrics |
| **CRAG Only** | LLM-based judgment | Human-like assessment |
| **LLM Only** | Custom evaluation criteria | Tailored to specific needs |
| **All Methods** | Comprehensive analysis | Complete evaluation coverage |
| **Search API + Evaluation** | End-to-end testing | Live system evaluation |

### Advanced UI Components

#### 1. **Dynamic Configuration Panel**
- Real-time validation of API credentials
- Smart defaults based on detected configurations
- Contextual help and tooltips
- Configuration templates for common setups

#### 2. **Interactive Progress Dashboard**
```javascript
// Real-time progress updates via WebSocket
{
  "session_id": "uuid",
  "progress": 75,
  "current_batch": 8,
  "total_batches": 10,
  "processing_speed": "1.2 queries/sec",
  "estimated_completion": "2 minutes",
  "success_rate": 94.5,
  "errors": 3
}
```

#### 3. **Advanced Result Visualization**
- **Radar Charts**: Multi-dimensional RAGAS metric visualization
- **Performance Heatmaps**: Query-level success/failure patterns
- **Time Series**: Processing speed over time
- **Comparison Views**: Side-by-side evaluation method results

#### 4. **Smart Error Recovery**
- Automatic retry with exponential backoff
- Partial result preservation
- Error categorization and suggestions
- Manual intervention options

### Session Management Features

#### 1. **Session Persistence**
```javascript
// Browser localStorage integration
const sessionData = {
  session_id: "uuid",
  created_at: "timestamp",
  last_file: "data.xlsx",
  preferences: {
    evaluation_method: "ragas+crag",
    performance_preset: "balanced"
  }
};
```

#### 2. **File History**
- Recently uploaded files
- Previous evaluation configurations
- Result download history
- Favorite settings templates

#### 3. **Auto-Save Configuration**
- Automatic saving of evaluation settings
- Quick restore of previous configurations
- Configuration sharing via URL parameters
- Export/import configuration files

### Real-time Monitoring

#### WebSocket Integration
```javascript
// Real-time updates
const ws = new WebSocket('ws://localhost:8001/ws/' + sessionId);
ws.onmessage = function(event) {
    const data = JSON.parse(event.data);
    updateProgress(data.progress);
    updateLogs(data.logs);
    updateMetrics(data.metrics);
};
```

#### Monitoring Metrics
- **Processing Speed**: Queries per second
- **API Response Times**: Average latency per API
- **Memory Usage**: Current and peak memory consumption
- **Error Rates**: Success/failure percentages
- **Queue Status**: Pending vs. completed batches

### Batch Processing Optimization

#### Intelligent Batching Algorithm
```python
# Adaptive batch sizing
def calculate_optimal_batch_size(api_latency, error_rate, memory_usage):
    base_size = 10
    
    # Adjust for API performance
    if api_latency > 5.0:  # High latency
        return max(base_size // 2, 3)
    elif api_latency < 1.0:  # Low latency
        return min(base_size * 2, 20)
    
    # Adjust for error rate
    if error_rate > 0.1:  # High error rate
        return max(base_size // 2, 3)
    
    # Adjust for memory usage
    if memory_usage > 0.8:  # High memory usage
        return max(base_size // 2, 3)
    
    return base_size
```

#### Retry Logic
- **Exponential Backoff**: Progressive delay between retries
- **Circuit Breaker**: Automatic failure detection and recovery
- **Fallback Strategies**: Alternative processing methods
- **Partial Success**: Continue processing despite some failures

### Result Analytics

#### Statistical Analysis
- **Distribution Analysis**: Score distribution across queries
- **Correlation Analysis**: Relationships between metrics
- **Outlier Detection**: Identification of unusual results
- **Trend Analysis**: Performance patterns over time

#### Export Options
- **Excel with Multiple Sheets**: Organized results
- **CSV for Analysis**: Raw data for external tools
- **JSON for Integration**: API-friendly format
- **PDF Reports**: Professional summary documents

#### Visualization Types
- **Scatter Plots**: Correlation between metrics
- **Box Plots**: Distribution analysis
- **Heat Maps**: Query vs. metric performance
- **Time Series**: Processing performance trends

## üÜò Support

### Getting Help
1. **Check Logs**: Review console output for detailed errors
2. **API Documentation**: Visit `/api/docs` for API details
3. **Health Check**: Use `/api/health` to verify system status
4. **GitHub Issues**: Report bugs and feature requests

### System Requirements
- **Python**: 3.8 or higher
- **Memory**: 4GB RAM minimum, 8GB recommended
- **Storage**: 1GB free space for results
- **Browser**: Modern browser with JavaScript enabled

## üîÑ Updates & Roadmap

### Version 2.0 Features ‚úÖ
- Asynchronous batch processing
- Modern web interface
- Real-time progress tracking
- Enhanced error handling
- Performance optimization

### Upcoming Features üöß
- WebSocket real-time updates
- Advanced filtering options
- Custom evaluation metrics
- API key management interface
- Results comparison tools

## üéØ Best Practices

### Data Preparation
1. **Quality Control**: Review data for accuracy before evaluation
2. **Consistent Format**: Maintain uniform column structure
3. **Balanced Dataset**: Include diverse query types and difficulties
4. **Context Quality**: Ensure context is relevant and well-formatted
5. **Ground Truth**: Provide comprehensive and accurate reference answers

### Evaluation Strategy
1. **Start Small**: Test with small datasets first
2. **Use Multiple Methods**: Combine RAGAS, CRAG, and LLM evaluation
3. **Monitor Performance**: Watch processing speed and error rates
4. **Iterative Improvement**: Use results to improve your RAG system
5. **Document Results**: Keep detailed records of evaluation runs

### Performance Optimization
1. **Right-size Settings**: Match performance settings to your infrastructure
2. **API Management**: Monitor rate limits and costs
3. **Error Handling**: Plan for partial failures and retries
4. **Resource Monitoring**: Watch memory and CPU usage
5. **Network Optimization**: Ensure stable, fast internet connection

## üìã Quick Reference

### UI Shortcuts
| Action | Shortcut | Description |
|--------|----------|-------------|
| **Upload File** | Drag & Drop | Quick file upload |
| **Start Evaluation** | Ctrl+Enter | Begin processing |
| **Toggle Settings** | Alt+S | Show/hide advanced settings |
| **Download Results** | Ctrl+D | Download latest results |
| **Clear Session** | Ctrl+Shift+C | Reset current session |

### Configuration Templates

**Quick Start (RAGAS Only):**
```json
{
  "params": {
    "evaluate_ragas": true,
    "evaluate_crag": false,
    "evaluate_llm": false,
    "batch_size": 10,
    "max_concurrent": 5
  }
}
```

**Comprehensive Evaluation:**
```json
{
  "params": {
    "evaluate_ragas": true,
    "evaluate_crag": true,
    "evaluate_llm": true,
    "use_search_api": true,
    "batch_size": 5,
    "max_concurrent": 2
  }
}
```

**High Performance:**
```json
{
  "params": {
    "evaluate_ragas": true,
    "evaluate_crag": false,
    "batch_size": 20,
    "max_concurrent": 10
  }
}
```

### API Endpoints Quick Reference

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/api/create-session` | POST | Create new session |
| `/api/get-sheet-names` | POST | Extract Excel sheets |
| `/api/runeval` | POST | Run evaluation |
| `/api/download-results/{session_id}` | GET | Download results |
| `/api/health` | GET | System health check |
| `/api/session-status/{session_id}` | GET | Session information |

## üîÆ Future Enhancements

### Upcoming Features (v2.1)
- **WebSocket Real-time Updates**: Live progress streaming
- **Custom Metric Definition**: User-defined evaluation criteria
- **A/B Testing Framework**: Compare different RAG configurations
- **Advanced Visualization**: Interactive charts and graphs
- **Export Templates**: Customizable report formats

### Roadmap (v3.0)
- **Multi-model Support**: Support for local LLMs
- **Batch Job Scheduling**: Queue and schedule evaluations
- **Team Collaboration**: Shared workspaces and results
- **API Rate Optimization**: Smart rate limiting and queuing
- **Advanced Analytics**: Trend analysis and insights

## üèÜ Success Stories

### Performance Improvements
> *"Using RAG Evaluator's async processing, we reduced our evaluation time from 45 minutes to 8 minutes for 500 queries. The detailed metrics helped us identify and fix context retrieval issues."*
> 
> ‚Äî AI Team Lead, TechCorp

### Quality Insights
> *"The combination of RAGAS and CRAG metrics revealed that our system had high faithfulness but low relevancy. We used these insights to improve our retrieval algorithm."*
> 
> ‚Äî ML Engineer, DataScience Inc.

### Integration Success
> *"Integrating RAG Evaluator into our CI/CD pipeline has made RAG system quality monitoring automatic. We catch regressions before they reach production."*
> 
> ‚Äî DevOps Engineer, AIStartup

## üìû Community & Support

### Community Resources
- **Documentation**: Comprehensive guides and tutorials
- **GitHub Discussions**: Community Q&A and feature requests
- **Example Datasets**: Sample data for testing and learning
- **Video Tutorials**: Step-by-step walkthrough videos
- **Best Practices Guide**: Industry-specific recommendations

### Contributing
We welcome contributions from the community:

1. **Bug Reports**: Help us identify and fix issues
2. **Feature Requests**: Suggest new capabilities
3. **Code Contributions**: Submit pull requests
4. **Documentation**: Improve guides and examples
5. **Testing**: Help test new features and releases

### Getting Support
1. **Documentation**: Check README and UI_README first
2. **GitHub Issues**: Report bugs and request features
3. **Community Discussions**: Get help from other users
4. **Email Support**: Contact maintainers for critical issues
5. **Professional Services**: Enterprise support available

---

**üöÄ Ready to revolutionize your RAG evaluation? Start now with `python start_ui.py` and experience the power of comprehensive, fast, and reliable RAG system assessment!**

**üìö For complete setup and configuration details, see the main [README.md](README.md)** 