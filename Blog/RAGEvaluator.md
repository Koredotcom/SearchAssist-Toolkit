
# RAGEvaluator: Simplifying the Evaluation of Retrieval-Augmented Generation Applications

## Introduction

In the rapidly evolving world of artificial intelligence, Retrieval-Augmented Generation (RAG) has emerged as a game-changer. RAG combines the power of large language models (LLMs) with external knowledge retrieval, enhancing the accuracy and relevance of AI-generated responses. But with great power comes great responsibility - how do we ensure that our RAG applications are performing as expected?

Enter [RAGEvaluator](), a powerful tool designed to assess and optimize RAG applications. In this blog post, we'll explore how RAGEvaluator works, why it's crucial for developers and data scientists, and how you can use it to improve your RAG-based systems.

## Introducing RAGEvaluator: A RAG Evaluation Solution

RAGEvaluator is an open-source tool that automates the process of evaluating RAG applications. It's designed to help developers, data scientists, and AI engineers measure the performance of their RAG systems across various dimensions.

Key features of RAGEvaluator include:

- Automated evaluation of retrieval quality
- Assessment of response relevance and coherence
- Measurement of hallucination rates
- Easy-to-understand performance reports

Whether you're fine-tuning a RAG system for a chatbot, developing a question-answering application, or building an AI-powered research assistant, RAGEvaluator provides valuable insights to guide your development process.

## Understanding RAG Evaluation

Before we dive into the specifics of RAGEvaluator, let's briefly discuss why evaluating RAG applications is so important.

RAG systems combine two critical components: a retrieval mechanism that fetches relevant information from a knowledge base, and a generation mechanism that produces human-like responses based on this information. While this approach can lead to more accurate and informative AI responses, it also introduces potential points of failure:

1. **Retrieval errors**: The system might fail to fetch the most relevant information.
2. **Integration issues**: The retrieved information might not be properly incorporated into the generated response.
3. **Hallucinations**: The LLM might generate false or irrelevant information not supported by the retrieved data.

Evaluating these aspects helps ensure that your RAG application is not just functional, but truly effective and reliable.

## How RAGEvaluator Works

RAGEvaluator operates by running your RAG application through a series of carefully designed test cases. Here's a simplified overview of the process:

1. **Input Processing**: RAGEvaluator takes in your RAG application details, test dataset, and configuration settings.
2. **Query Execution**: It runs a set of predefined queries through your RAG system.
3. **Response Analysis**: The tool analyzes the responses generated by your RAG application.
4. **Metric Calculation**: Various performance metrics are calculated based on the analysis.
5. **Report Generation**: A comprehensive report is produced with the evaluation results.

The RAG Evaluator assesses a RAG application using the widely recognized RAGAS and CRAG benchmarks. The evaluation encompasses several key metrics:

### RAGAS Metrics:
### Faithfulness
This measures the factual consistency of the generated answer against the given context. It is calculated from the answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.

**Question:** Where and when was Einstein born?  
**Context:** Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.

- **High faithfulness answer:** Einstein was born in Germany on 14th March 1879.
- **Low faithfulness answer:** Einstein was born in Germany on 20th March 1879.

### Answer Relevance
The evaluation metric, Answer Relevancy, focuses on assessing how relevant the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. This metric is computed using the question, the context and the answer. Please note, that even though in practice the score will range between 0 and 1 most of the time, this is not mathematically guaranteed, due to the nature of the cosine similarity ranging from -1 to 1.

**Question:** Where is France and what is its capital?  
- **Low relevance answer:** France is in western Europe.
- **High relevance answer:** France is in western Europe and Paris is its capital.

### Context Precision
Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally, all the relevant chunks must appear at the top ranks. This metric is computed using the question, ground truth, and the contexts, with values ranging between 0 and 1, where higher scores indicate better precision.

**Question:** Where is France and what is its capital?  
**Ground truth:** France is in Western Europe and its capital is Paris.

- **High context precision:** 
  - "France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower."
  - "The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and the vast Palace of Versailles attest to its rich history."

- **Low context precision:** 
  - "The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and..."
  - "France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower."

### Context Relevance
This metric gauges the relevancy of the retrieved context, calculated based on both the question and contexts. The values fall within the range of (0, 1), with higher values indicating better relevancy.

**Question:** What is the capital of France?  
- **High context relevance:** "France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower."
- **Low context relevance:** "France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower. The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and the vast Palace of Versailles attest to its rich history."

### Context Recall
Context recall measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. It is computed based on the ground truth and the retrieved context, and the values range between 0 and 1, with higher values indicating better performance.

**Question:** Where is France and what is its capital?  
**Ground truth:** France is in Western Europe and its capital is Paris.

- **High context recall:** "France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower."
- **Low context recall:** "France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and the vast Palace of Versailles attest to its rich history."

### Answer Semantic Similarity
The concept of Answer Semantic Similarity pertains to the assessment of the semantic resemblance between the generated answer and the ground truth. This evaluation is based on the ground truth and the answer, with values falling within the range of 0 to 1. A higher score signifies a better alignment between the generated answer and the ground truth.

**Ground truth:** "Albert Einstein’s theory of relativity revolutionized our understanding of the universe."  
- **High similarity answer:** "Einstein’s groundbreaking theory of relativity transformed our comprehension of the cosmos."
- **Low similarity answer:** "Isaac Newton’s laws of motion greatly influenced classical physics."

### Answer Correctness
The assessment of Answer Correctness involves gauging the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness.

**Ground truth:** "Einstein was born in 1879 in Germany."  
- **High answer correctness:** "In 1879, Einstein was born in Germany."
- **Low answer correctness:** "Einstein was born in Spain in 1879."

###CRAG Metrics:
Need to add

These metrics provide a holistic view of your RAG application's performance, helping you identify areas for improvement.

##Getting Started with RAGEvaluator

To begin using RAGEvaluator, you'll need to have the following prerequisites:

### Prerequisites

- Install a new python virtual environment(Recommended)
- Python 3.9.x
- Pip package manager

### Installing Packages

1. Ensure you have Python and pip installed. You can check this by running:
 ```sh
 python --version
 pip --version
 ```

2. Install the necessary packages by running:
 ```sh
 pip install -r requirements.txt
 ```


## Running Your First Evaluation

Let's walk through a basic example of how to use RAGEvaluator:


To run an evaluation on a Search AI application, follow these steps:

1. Prepare your Excel file with the following columns:
    - `query`: The query string.
    - `ground_truth`: The expected ground truth for the query.

2. Execute the script with the following command:

```sh
python main.py --input_file path/to/your/excel_file.xlsx --sheet_name "Sheet1" --use_search_api
```


This script will run your RAG application through the evaluation process and output the results.

## Best Practices and Tips

To get the most out of RAGEvaluator:

1. Use a diverse set of test queries that represent real-world usage of your application.
2. Regularly re-evaluate your RAG application, especially after making changes to your model or knowledge base.
3. Use RAGEvaluator in conjunction with human evaluation for a more comprehensive assessment.

## Use Cases and Examples

RAGEvaluator can be particularly useful in scenarios such as:

- Comparing different retrieval algorithms for your RAG system
- Assessing the impact of expanding or refining your knowledge base
- Monitoring the performance of a deployed RAG application over time
- Fine-tuning the balance between retrieval and generation in your responses

## Conclusion

RAGEvaluator provides a powerful, automated way to assess and improve your RAG applications. By offering detailed insights into various aspects of performance, it enables developers and data scientists to build more reliable, accurate, and effective AI systems.

Whether you're just starting with RAG or looking to optimize an existing application, RAGEvaluator can be an invaluable tool in your AI development toolkit. Start evaluating your RAG applications today and unlock the full potential of retrieval-augmented generation!
